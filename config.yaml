wandb:
  entity: 
  project: pokemon
  group: ~

env:
  # Directory with pyboy save states to load initial state from.
  state_dir: environment/states
  # File within state dir to load from. Overrides init_from_last_ending_state.
  # Leave blank and set init_from_last_ending_state to True to load from last ending state.
  override_init_state: 
  # Set to True to load most-recent ending save state. Overridden by override_init_state if set.
  init_from_last_ending_state: True
  # Allow human player input
  interactive_mode: True
  # If true, record replays.
  record_replays: True
  # If true, run without rendering every step.
  headless: False
  # Directory to store video output.
  video_dir: video
  # Hold button down for this many ticks (+1)
  emulator_delay: 11
  # How many ticks each environment step should take.
  action_freq: 24
  # Maximum number of steps per mini-episode. Not applicable - holdover from RL.
  max_steps: 19816
  # If true, save videos.
  save_video: False
  # If true, save sped up videos.
  fast_video: False
  # Number of environments to save videos for. Not applicable - holdover from RL.
  n_record: 10
  # If true, give all party members max base stats.
  perfect_ivs: True
  # If true, downsample the screen observation by 2.
  reduce_res: False
  # If true, losslessly compress the screen observation.
  two_bit: True
  # How many steps until the environment returns an info. Not applicable - holdover from RL.
  log_frequency: 2000
  # If true, disable the need for Flash.
  auto_flash: False
  # If true, disable wild encounters with repels.
  disable_wild_encounters: True
  # If true, disable AI input. Useful for debugging.
  disable_ai_actions: True
  # If true, teach cut if possible.
  auto_teach_cut: True
  # If true, use cut when standing next to a tree.
  auto_use_cut: True
  # If true, use surf when attempting to walk on water.
  auto_use_surf: True
  # If true, teach surf if possible.
  auto_teach_surf: True
  # If true, use strength when walking into a cave if possible.
  auto_use_strength: True
  # If true, teach strength if possible.
  auto_teach_strength: True
  # If true, solve a strength puzzle if next to a moveable boulder.
  auto_solve_strength_puzzles: True
  # If true, toss all unneeded items when the bag is full.
  auto_remove_all_nonuseful_items: False
  # If true, use Pokeflute when next to a Snorlax if possible.
  auto_pokeflute: True
  # If true, go to the next floor module num floors when entering an elevator.
  auto_next_elevator_floor: True
  # If true, automatically give the player the GOLD TEETH and HM03.
  # The player will not be allowed in the Safari Zone.
  skip_safari_zone: False
  # If true, the player will have no cap on the number of steps when in the
  # Safari Zone
  infinite_safari_steps: False
  # If true, place a Lemonade in the player's bag if there is no Lemonade when
  # in the Celadon Mart.
  insert_saffron_guard_drinks: False
  # If true, give the player infinite money.
  infinite_money: True
  # If true, all Party Pokemon will have a fixed HP of 128.
  infinite_health: True
  # If true, use the global map observation.
  use_global_map: False
  # If true, provide the PyBoy save state as a part of infos. (Actually does other things now)
  save_state: True
  # If true, animate scripts. Script animations will not be a part of obs.
  # This flag is good for debugging.
  animate_scripts: True
  # The value on the exploration map for visiting a new coordinate. Not applicable - holdover from RL.
  exploration_inc: 1.0
  # The max possible value on the exploration map. Not applicable - holdover from RL.
  exploration_max: 1.0
  # The amount to scale max steps by. Will be multiplied by the number of required items and events. Not applicable - holdover from RL.
  max_steps_scaling: 0 # 0.2 # every 10 events or items gained, multiply max_steps by 2
  # The scaling for visiting important map IDs. Not applicable - holdover from RL.
  map_id_scalefactor: 9.539340019226074 # multiply map ids whose events have not been completed by 10
  # How many agents must reach a required event before all agents are allowed to continue. Not applicable - holdover from RL.
  # If an agent reaches a milestone and the required tolerance is not met, the episode will reset. Not applicable - holdover from RL.
  required_tolerance: null
  # required_tolerance: 0.02 # Not applicable - holdover from RL.

train:
  # Random seed. Not applicable - holdover from RL.
  seed: 1
  # Use deterministic Torch algorithms. Not applicable - holdover from RL.
  torch_deterministic: True
  # Device to run training on. Not applicable - holdover from RL.
  device: cuda
  # If true, use torch.compile. Not applicable - holdover from RL.
  compile: False
  # The mode for torch compilation. Not applicable - holdover from RL.
  compile_mode: "reduce-overhead"
  # Mostly useless for Pokemon, enables tensor cores on NVIDIA GPUs. Not applicable - holdover from RL.
  float32_matmul_precision: "high"
  # The total time steps to run training for.
  total_timesteps: 10_000_000_000 # 100_000_000_000 for full games. Not applicable - holdover from RL.
  # The number of examples for one epoch of training.
  batch_size: 65536 # Not applicable - holdover from RL.
  # The number of example to train for one batch of training.
  minibatch_size: 2048 # Not applicable - holdover from RL.
  # If true, turn on learning rate annealing. Not applicable - holdover from RL.
  anneal_lr: False
  # Unused. Not applicable - holdover from RL.
  num_minibatches: 4 # Not applicable - holdover from RL.
  # Number of epochs when training. Not applicable - holdover from RL.
  update_epochs: 3
  # If true, use normalized advantage. Not applicable - holdover from RL.
  norm_adv: True
  # Entropy coefficient. More means more random actions. Not applicable - holdover from RL.
  ent_coef: 0.01001337416019442
  # Generalized advantage estimate. Not applicable - holdover from RL.
  gae_lambda: 0.949936199629264
  # Decay parameter for advantage estimation. Not applicable - holdover from RL.
  gamma: 0.9979900678597386
  # How strongly to clip gradients. Not applicable - holdover from RL.
  clip_coef: 0.1 # Not applicable - holdover from RL.
  clip_vloss: True # Not applicable - holdover from RL.
  # Learning rate. Not applicable - holdover from RL.
  learning_rate: 0.00019999482124740103
  # Max norm of the gradients. Not applicable - holdover from RL.
  max_grad_norm: 0.5 # Not applicable - holdover from RL.
  target_kl: ~ # Not applicable - holdover from RL.
  vf_clip_coef: 0.1 # Not applicable - holdover from RL.
  vf_coef: 0.5319333384064214 # Not applicable - holdover from RL.
  # unused. Not applicable - holdover from RL.
  batch_rows: 128 # Not applicable - holdover from RL.
  # backprop through time horizon. Not applicable - holdover from RL.
  bptt_horizon: 16 # Not applicable - holdover from RL.

  # Number of environments for training. Not applicable - holdover from RL.
  num_envs: 288 # Not applicable - holdover from RL.
  # Number of workers (processes) for training. Not applicable - holdover from RL.
  # Environments are divided evenly across workers. Not applicable - holdover from RL.
  num_workers: 24 # Not applicable - holdover from RL.
  # Number of workers to report per step. Not applicable - holdover from RL.
  env_batch_size: 36 # Not applicable - holdover from RL.
  # Use PufferLib EnvPool. Not applicable - holdover from RL.
  env_pool: True # Not applicable - holdover from RL.
  # Use PufferLib's zero copy EnvPool. Not applicable - holdover from RL.
  zero_copy: False # Not applicable - holdover from RL.

  # Verbose terminal reporting.
  verbose: False
  # Directory to store checkpoints in.
  data_dir: runs
  # If true, save checkpoints.
  save_checkpoint: False
  # How often to save checkpoints.
  checkpoint_interval: 200

  # Log global map overlay to wandb 
  save_overlay: True
  # How often to log global map to wandb.
  overlay_interval: 100
  # Use PufferLib's CPU data offloading.
  cpu_offload: False
  # Use RNN network. False is not supported. Not applicable - holdover from RL.
  use_rnn: True
  # Use multiprocessing to asynchronously reset the agents
  # when required new events and items are observed. Not applicable - holdover from RL.
  async_wrapper: False
  # Use a shared SQLite DB to reset the agents
  # when required new events and items are observed. Not applicable - holdover from RL.
  sqlite_wrapper: True
  # Save PyBoy states when new required events and items are observed. Progress tracking not implemented.
  archive_states: True
  # When a new required events and items are observed, move
  # all environments to the save state for the environment that made 
  # the observation
  swarm: True
  # event name: minutes. If we dont satisfy each condition
  # we early stop
  # The defaults have a margin of error
  early_stop:
    EVENT_BEAT_BROCK: 60
    EVENT_BEAT_MISTY: 600
    EVENT_GOT_HM01: 1200
    EVENT_BEAT_LT_SURGE: 2400
  # If set, train until all events are accomplished.
  one_epoch:
    - "EVENT_BEAT_CHAMPION_RIVAL"
    # - HM_03
    # - HM_04
    # - "EVENT_BEAT_ERIKA"
  # If true, provide the percentage completion rate for required
  # items to the environments.
  required_rate: False

# Wrappers to use for training.
# Each wrapper is keyed by a name for the commandline.
# The value for each key is the list of wrappers to support.
# Wrappers can be found in the wrappers directory.
wrappers:
  empty:
    - episode_stats.EpisodeStatsWrapper: {}

  recorder:
    - coords_writer.CoordinatesWriter:
        output_dir: runs
        write_frequency: 1000

  baseline:
    - stream_wrapper.StreamWrapper:
        user: xinpw8
    - exploration.DecayWrapper:
        step_forgetting_factor:
          npc: 0.995
          coords: 0.9995
          map_ids: 0.995
          explore: 0.9995
          start_menu: 0.998
          pokemon_menu: 0.998
          stats_menu: 0.998
          bag_menu: 0.998
          action_bag_menu: 0.998
        forgetting_frequency: 10
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 0

  finite_coords:
    - stream_wrapper.StreamWrapper:
        user: 
    - exploration.MaxLengthWrapper:
        capacity: 1750
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 0

  stream_only_with_coords:
    - coords_writer.ActionsWriter:
        output_dir: runs
        write_frequency: 1000
    - coords_writer.CoordinatesWriter:
        output_dir: runs
        write_frequency: 1000
    - stream_wrapper.StreamWrapper:
        user:
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 10
        jitter: 5

  stream_only:
    - stream_wrapper.StreamWrapper:
        user:
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 1

  fixed_reset_value:
    - stream_wrapper.StreamWrapper:
        user:
    - exploration.OnResetLowerToFixedValueWrapper:
        fixed_value:
          coords: 0.33
          map_ids: 0.33
          npc: 0.33
          cut: 0.33
          explore: 0.33
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 25
        jitter: 0
    - episode_stats.EpisodeStatsWrapper: {}

# Rewards to use for training.
# Each reward is keyed by a name for the commandline.
# The value for each key is the list of rewards to support.
# Rewards can be found in the rewards directory.
rewards:
  baseline.BaselineRewardEnv:
    reward:
  baseline.TeachCutReplicationEnv:
    reward:
      event: 1.0
      bill_saved: 5.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      obtained_move_ids: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 10.0
      exploration: 0.02
      cut_coords: 1.0
      cut_tiles: 1.0
      start_menu: 0.01
      pokemon_menu: 0.1
      stats_menu: 0.1
      bag_menu: 0.1

  baseline.TeachCutReplicationEnvFork:
    reward:
      event: 1.0
      bill_saved: 5.0
      obtained_move_ids: 4.0
      hm_count: 10.0
      badges: 10.0
      exploration: 0.02
      cut_coords: 1.0
      cut_tiles: 1.0
      start_menu: 0.01
      pokemon_menu: 0.1
      stats_menu: 0.1
      bag_menu: 0.1
      taught_cut: 10.0
      explore_npcs: 0.02
      explore_hidden_objs: 0.02

  baseline.CutWithObjectRewardsEnv:
    reward:
      event: 1.0
      bill_saved: 5.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      obtained_move_ids: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 10.0
      exploration: 0.02
      cut_coords: 0.0
      cut_tiles: 0.0
      start_menu: 0.00
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.1
      rocket_hideout_found: 5.0
      explore_hidden_objs: 0.02
      seen_action_bag_menu: 0.1

  baseline.CutWithObjectRewardRequiredEventsEnv:
    reward:
      event: 1.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      obtained_move_ids: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 5.0
      exploration: 0.02
      cut_coords: 0.0
      cut_tiles: 0.0
      start_menu: 0.0
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.0
      explore_hidden_objs: 0.02
      seen_action_bag_menu: 0.0
      required_event: 5.0
      required_item: 5.0
      useful_item: 1.0
      pokecenter_heal: 1.0

  baseline.ObjectRewardRequiredEventsEnvTilesetExploration:
    reward:
      event: 1.0
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      obtained_move_ids: 4.0
      hm_count: 10.0
      level: 1.0
      badges: 5.0
      cut_coords: 0.0
      cut_tiles: 0.0
      start_menu: 0.0
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.0
      explore_hidden_objs: 0.01
      explore_signs: 0.015
      seen_action_bag_menu: 0.0
      required_event: 5.0
      required_item: 5.0
      useful_item: 1.0
      pokecenter_heal: 0.2
      exploration: 0.02
      exploration_gym: 0.025
      exploration_facility: 0.11
      exploration_plateau: 0.025
      exploration_lobby: 0.035 # for game corner
      a_press: 0.0 # 0.00001
      explore_warps: 0.05
      use_surf: 0.05

  baseline.ObjectRewardRequiredEventsMapIds:
    reward:
      a_press: 0.0 # 0.00001
      badges: 3.0
      bag_menu: 0.0
      caught_pokemon: 2.5
      valid_cut_coords: 1.5
      event: .75
      exploration: 0.018999755680454297
      explore_hidden_objs: 0.00009999136567868017
      explore_signs: 0.015025767686371013
      explore_warps: 0.010135211705238394
      hm_count: 7.5
      invalid_cut_coords: 0.0001
      level: 1.05
      obtained_move_ids: 4.0
      pokecenter_heal: 0.47
      pokemon_menu: 0.0
      required_event: 7.0
      required_item: 3.0
      seen_action_bag_menu: 0.0
      seen_pokemon: 2.5
      start_menu: 0.0
      stats_menu: 0.0
      use_surf: 0.4
      useful_item: 0.825
      safari_zone: 3.4493650422686217

  baseline.ObjectRewardRequiredEventsMapIdsFieldMoves:
    reward:
      a_press: 0.0 # 0.00001
      badges: 1.9426719546318056
      bag_menu: 0.0981288719177246
      caught_pokemon: 3.076385498046875
      cut_tiles: 0.9939162135124208
      event: 0.7267916798591614
      exploration: 0.02902716636657715
      explore_hidden_objs: 0.00000101456356048584
      explore_signs: 0.01442893123626709
      explore_warps: 0.01322316527366638
      hm_count: 8.083643913269043
      invalid_cut_coords: 0.0012159876823425292
      invalid_pokeflute_coords: 0.0010716800689697266
      invalid_surf_coords: 0.0010805776119232175
      level: 1.098945999145508
      obtained_move_ids: 4.091134548187256
      pokecenter_heal: 0.75 # 0.5932707786560059
      pokemon_menu: 0.09460494995117188
      pokeflute_tiles: 0.9939162135124208
      required_event: 7.134671688079834
      required_item: 3.3519155979156494
      safari_zone: 55.1411423683 # (100.0 + 10.2822847366)/2
      seen_action_bag_menu: 0.0
      seen_pokemon: 2.1898984909057617
      start_menu: 0.010755150794982913
      stats_menu: 0.09938607215881348
      surf_tiles: 0.9939162135124208
      use_surf: 0.0
      useful_item: 1.3436599969863892
      use_ball_count: -10.0
      valid_cut_coords: 4.822387218475342
      valid_pokeflute_coords: 4.840555191040039
      valid_surf_coords: 4.8967742919921875

# Policies to use for training.
# Each policy is keyed by a name for the commandline.
# The value for each key is the list of policies to support.
# Policies can be found in the policies directory.
policies:
  multi_convolutional.MultiConvolutionalPolicy:
    policy:
      hidden_size: 512

    rnn:
      # Assumed to be in the same module as the policy
      name: MultiConvolutionalRNN
      args:
        input_size: 512
        hidden_size: 512
        num_layers: 1

# Debug overrides.
debug:
  env:
    headless: False
    stream_wrapper: False
    init_state: Bulbasaur
    state_dir: pyboy_states
    max_steps: 19000
    log_frequency: 1
    disable_ai_actions: True
    use_global_map: False
    reduce_res: True
    animate_scripts: True
    save_state: False
    save_video: True
    fast_video: False
    auto_flash: True
    disable_wild_encounters: True
    auto_teach_cut: False
    auto_use_cut: False
    auto_use_strength: False
    auto_use_surf: False
    auto_teach_surf: False
    auto_teach_strength: False
    auto_solve_strength_puzzles: False
    auto_remove_all_nonuseful_items: False
    auto_pokeflute: False
    auto_next_elevator_floor: False
    skip_safari_zone: False
    infinite_safari_steps: False
    infinite_health: True
    insert_saffron_guard_drinks: False
    perfect_ivs: False
    two_bit: False
  train:
    device: cpu
    compile: False
    compile_mode: default
    num_envs: 1
    envs_per_worker: 1
    num_workers: 1
    env_batch_size: 128
    zero_copy: False
    batch_size: 1024
    minibatch_size: 128
    batch_rows: 4
    bptt_horizon: 2
    total_timesteps: 1_000_000
    save_checkpoint: True
    checkpoint_interval: 4
    save_overlay: True
    overlay_interval: 1
    verbose: True
    env_pool: False
    load_optimizer_state: False
    async_wrapper: False
    sqlite_wrapper: False
    archive_states: False

env_config:
  # Path to your Pokemon Red ROM file
  gb_path: "/puffertank/grok_plays_pokemon/environment/red.gb"
  # Optional: set headless to true to disable Pygame window
  headless: false

# New section for Agent and LLM configuration
agent:
  # Specify the LLM provider (e.g., 'grok', 'anthropic', 'google', 'openai')
  # For this project, we will focus on 'grok'
  llm_provider: "grok" 
  
  # Model name for the specified LLM provider
  model_name: "grok-3-mini"

  # Maximum tokens for LLM responses
  max_tokens: 2048

  # Temperature for LLM sampling (0.0 to 1.0)
  temperature: 0.7

  # API Key for the LLM provider (can also be set via environment variable)
  # e.g., GROK_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, OPENAI_API_KEY
  # If set here, it might override environment variables depending on loading logic.
  # It's often better to use environment variables for keys.
  # api_key: "YOUR_GROK_API_KEY_IF_NOT_USING_ENV_VAR" 